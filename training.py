# -*- coding: utf-8 -*-
"""Roberta-base-training-trilogue-pos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G6sJptcknBAeLOsmN0WulGpidI023cTR
"""

import datetime
import os
import time
import sys
import re
import numpy as np
import random
import pandas as pd
import nltk
import numpy as np
import json
import wandb
import torch
from torch import nn
from sklearn.utils.class_weight import compute_class_weight
from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler
torch.manual_seed(50)

from datasets.dataset_dict import DatasetDict
from datasets import load_metric, load_dataset 
from transformers import RobertaTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments


nltk.download('punkt')

#POSITION = 'Council'
base_path = '/work/sobaidul/trilog_classifier/trilog_training_data_all_clean'


#device = "cuda:0" if torch.cuda.is_available() else "cpu"
device = 'cuda:0'
def preprocess_function(batch):
    return tokenizer(batch['text'], truncation=True, padding="max_length")

def compute_metrics(eval_pred):
    f1_score = load_metric("f1")
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    f1_score.add_batch(predictions=predictions, references=labels)
    return f1_score.compute(average="macro")

class CustomTrainerUpScaling(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get('logits')
        # compute custom loss
        #class_weights = compute_class_weight(class_weight='balanced', classes= np.unique(y), y = y)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([3.34294452, 1.47267129, 0.49460281])).to(device)
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

class CustomTrainerDownScaling(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get('logits')
        # compute custom loss
        #class_weights = compute_class_weight(class_weight='balanced', classes= np.unique(y), y = y)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.0, 0.4 ])).to(device)
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

train_data = load_dataset('csv', data_files=f'{base_path}/all_train_clean_new.csv', sep=',',  keep_default_na=False)
#val_data = load_dataset('csv', data_files=f'{base_path}/all_val.csv', sep=',',  keep_default_na=False)
test_data = load_dataset('csv', data_files=f'{base_path}/all_test_clean_gold.csv', sep=',',  keep_default_na=False)
dataset = DatasetDict({
    'train': train_data['train'],
    'test': test_data['train']})
dataset = dataset.remove_columns(['Unnamed: 0','original_label','identifier','column'])

model_checkpoint = "roberta-large"
tokenizer = RobertaTokenizer.from_pretrained("roberta-large")
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

tokenized_data = dataset.map(preprocess_function, batched=True)
print(tokenized_data)
 
training_args = TrainingArguments(
    output_dir=f"{base_path}/results_roberta_trilog_all_upscaling-pos-new",  # output directory
    num_train_epochs=30,  # total # of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=16,  # batch size for evaluation
    warmup_steps=1000,  # number of warmup steps for learning rate scheduler
    weight_decay=0.01,  # strength of weight decay
    learning_rate=2e-5,  # learning rate
    save_total_limit=2,  # limit the total amount of checkpoints, delete the older checkpoints
    logging_dir="./logs",  # directory for storing logs
    logging_steps=1000,
    evaluation_strategy="steps",
    eval_steps=1000,
    save_strategy="epoch",
    save_steps=1500,
    report_to='wandb'
)
 
trainer = CustomTrainerUpScaling(
    model=model,  # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,  # training arguments, defined above
    train_dataset=tokenized_data["train"],  # training dataset
    eval_dataset=tokenized_data["test"],  # evaluation dataset
    compute_metrics=compute_metrics,  # the callback that computes metrics of interest
)

trainer.train()

trainer.save_model(f'{base_path}/Roberta-large-upscaling-pos-new')

'''
#inference
predictions = trainer.predict(test_dataset=tokenized_data["test"])
preds = predictions.predictions.argmax(-1)
labels = pd.Series(preds).map(model.config.id2label)
scores = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)
df = pd.read_csv(f'{base_path}/all_test_clean.csv', sep=',')
df['prediction'] = preds
df['score'] = scores

df.to_csv(f'{base_path}/result_test_all_upcaling_clean_2.csv', sep=',', index=False)
'''